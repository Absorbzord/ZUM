{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbc9fcd",
   "metadata": {},
   "source": [
    "Czyszczenie danych, preprocessing, inżynieria cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b786d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad448e0",
   "metadata": {},
   "source": [
    "# wczytanie dancyh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c25c2aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1818, 10), (227, 10), (228, 10))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_parquet(\"../data/processed/train.parquet\")\n",
    "val_df = pd.read_parquet(\"../data/processed/val.parquet\")\n",
    "test_df = pd.read_parquet(\"../data/processed/test.parquet\")\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7336c20",
   "metadata": {},
   "source": [
    "# Podział na X i y\n",
    "Dzięki czemu mam spójne dane na wszystkich modelach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76358a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df[\"input_text\"].astype(str)\n",
    "X_val = val_df[\"input_text\"].astype(str)\n",
    "X_test = test_df[\"input_text\"].astype(str)\n",
    "\n",
    "y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val = val_df[\"label\"].astype(int).to_numpy()\n",
    "y_test = test_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "np.unique(y_train), np.unique(y_val), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89ccf4",
   "metadata": {},
   "source": [
    "# Mapowanie etykiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5562fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'low', 1: 'mid', 2: 'high'}, {'low': 0, 'mid': 1, 'high': 2})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {0: \"low\", 1: \"mid\", 2: \"high\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bc530",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "wybrałem TF-IDF, ponieważ daje silny baseline i działa bez GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9304965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1818, 5496), (227, 5496), (228, 5496))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf   = tfidf.transform(X_val)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3f155",
   "metadata": {},
   "source": [
    "# Dataset dla PyTorch\n",
    "Klasa z konwersją per-batch w funkcji `__getitem__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc6edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfDataset(Dataset):\n",
    "    def __init__(self, X_sparse, y):\n",
    "        self.X = X_sparse\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].toarray().ravel().astype(np.float32)\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[idx], dtype==torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fca8091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1818, 227, 228)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TfidfDataset(X_train_tfidf, y_train)\n",
    "val_ds = TfidfDataset(X_val_tfidf, y_val)\n",
    "test_ds = TfidfDataset(X_test_tfidf, y_test)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61116248",
   "metadata": {},
   "source": [
    "# Tokenizacja \n",
    "\n",
    "początkowo ustawiłem maks. długość na 256, ale po teście z komórki poniżej wynika, że można ustawić 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d54b69fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 3160, 102, 3437, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funkcja zwracająca stałe wartości dla tokenizowanych tekstów\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# określenie modelu\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer(\"question [SEP] answer\", truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ed65c",
   "metadata": {},
   "source": [
    "# ustalenie `max_length` na bazie EDA\n",
    "\n",
    "wynik array([32.  , 45.25, 57.  , 70.1 , 91.21]) mówi, że 99% danych mieści się w 91 tokenach. Czyli odpowiedzi są w większości dość zwięzłe.\n",
    "\n",
    "Uzasadnia to wybranie długości maksymalnej tokenów w celu minimalizacji truncation, szybszego treningu i mniejszego zużycia VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed637bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.  , 45.25, 57.  , 70.1 , 91.21])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = X_train.sample(200, random_state=67).tolist()\n",
    "lengths = [len(tokenizer(t, truncation=False)[\"input_ids\"]) for t in sample_texts]\n",
    "\n",
    "np.percentile(lengths, [50, 75, 90, 95, 99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84187d",
   "metadata": {},
   "source": [
    "## Podsumowanie etapu\n",
    "\n",
    "\n",
    "- Przygotowano reprezentację TF-IDF dla modeli klasycznych i sieci neuronowej.\n",
    "- Zaimplementowano kodowanie etykiet.\n",
    "- Przeprowadzono tokenizację danych wejściowych dla modelu transformerowego.\n",
    "- Dane są gotowe do etapu trenowania modeli.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
